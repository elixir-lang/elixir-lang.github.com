msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"PO-Revision-Date: 2017-10-30 15:10+0900\n"
"Language: ja\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"

msgid ""
"---\n"
"layout: post\n"
"title: Announcing GenStage\n"
"author: José Valim\n"
"category: Announcements\n"
"excerpt: GenStage is a new Elixir behaviour for exchanging events with back-pr"
"essure between Elixir processes. In this blog post we will cover the backgroun"
"d that led us to GenStage, some example use cases, and what we are exploring f"
"or future releases.\n"
"---"
msgstr ""

msgid ""
"Today we are glad to announce the official release of GenStage. GenStage is a "
"new Elixir behaviour for exchanging events with back-pressure between Elixir p"
"rocesses. In the short-term, we expect GenStage to replace the use cases for G"
"enEvent as well as providing a composable abstraction for consuming data from "
"third-party systems."
msgstr ""

msgid ""
"In this blog post we will cover the background that led us to GenStage, some e"
"xample use cases, and what we are exploring for future releases. If instead yo"
"u are looking for a quick reference, [check the project source code](https://g"
"ithub.com/elixir-lang/gen_stage) and [access its documentation](https://hexdoc"
"s.pm/gen_stage/Experimental.GenStage.html)."
msgstr ""

msgid "## Background"
msgstr ""

msgid ""
"One of the original motivations for [creating and designing Elixir was to intr"
"oduce better abstractions for working with collections](https://www.youtube.co"
"m/watch?v=Lqo9-pQuRKE). Not only that, we want to provide developers intereste"
"d in manipulating collections with a path to take their code from eager to laz"
"y, to concurrent and then distributed."
msgstr ""

msgid ""
"Let's discuss a simple but actual example: word counting. The idea of word cou"
"nting is to receive one file and count how many times each word appears in the"
" document. Using the `Enum` module it could be implemented as follows:"
msgstr ""

msgid ""
"```elixir\n"
"File.read!(\"path/to/some/file\")\n"
"|> String.split(\"\n"
"\")\n"
"|> Enum.flat_map(fn line ->\n"
"    String.split(line, \" \")\n"
"   end)\n"
"|> Enum.reduce(%{}, fn word, acc ->\n"
"    Map.update(acc, word, 1, & &1 + 1)\n"
"   end)\n"
"|> Enum.to_list()\n"
"```"
msgstr ""

msgid ""
"While the solution above works fine and is efficient for small files, it is qu"
"ite restrictive for large inputs as it loads the whole file into memory."
msgstr ""

msgid ""
"Another issue with the solution above is that the `Enum.flat_map/2` step will "
"build a huge list, with all the words in the file, before we effectively start"
" counting them. Again, for a large document, this means more memory usage and "
"a waste of processing time in building a list that will be traversed right aft"
"er."
msgstr ""

msgid ""
"Luckily, Elixir provides a solution to this problem (and has provided it for q"
"uite some time): streams. One of the advantage of streams is they are lazy, al"
"lowing us to traverse collections item by item, in this case, line by line, in"
"stead of loading the whole data set into memory. Let's rewrite the example abo"
"ve to use streams:"
msgstr ""

msgid ""
"```elixir\n"
"File.stream!(\"path/to/some/file\")\n"
"|> Stream.flat_map(fn line ->\n"
"    String.split(line, \" \")\n"
"   end)\n"
"|> Enum.reduce(%{}, fn word, acc ->\n"
"    Map.update(acc, word, 1, & &1 + 1)\n"
"   end)\n"
"|> Enum.to_list()\n"
"```"
msgstr ""

msgid ""
"By using `File.stream!` and `Stream.flat_map`, we build a lazy computation tha"
"t will emit a single line, break that line into words, and emit such words one"
" by one without building huge lists in memory when enumerated. The functions i"
"n the [Stream module](https://hexdocs.pm/elixir/Stream.html) just express the "
"computation we want to perform. The computation itself, like traversing the fi"
"le or breaking into words in `flat_map`, only happens when we call a function "
"in the `Enum` module. We have covered [the foundation for Enum and Streams](ht"
"tp://blog.plataformatec.com.br/2015/05/introducing-reducees/) in another artic"
"le."
msgstr ""

msgid ""
"The solution above allows us to work with large datasets without loading them "
"all into memory. For large files, it is going to provide much better performan"
"ce than the eager version. However, the solution above still does not leverage"
" concurrency. For a machine with more than one core, which is the huge majorit"
"y of machines we have available today, it is a suboptimal solution."
msgstr ""

msgid "That said, how could we leverage concurrency in the example above?"
msgstr ""

msgid ""
"During my ElixirConf 2015 keynote, [I discussed one of the most immediate solu"
"tions to this problem](http://confreaks.tv/videos/elixirconf2015-keynote) whic"
"h was to convert parts of your pipeline to separate processes:"
msgstr ""

msgid ""
"```elixir\n"
"File.stream!(\"path/to/some/file\")\n"
"|> Stream.flat_map(fn line ->\n"
"    String.split(line, \" \")\n"
"   end)\n"
"|> Stream.async()  # NEW!\n"
"|> Enum.reduce(%{}, fn word, acc ->\n"
"    Map.update(acc, word, 1, & &1 + 1)\n"
"   end)\n"
"|> Enum.to_list()\n"
"```"
msgstr ""

msgid ""
"The idea is that `Stream.async` would run the previous computations in a separ"
"ate process that would stream its messages to the process that called `Enum.re"
"duce`. Unfortunately, the solution above is less than ideal."
msgstr ""

msgid ""
"First of all, we want to avoid moving data between processes as much as possib"
"le. Instead, we want to start multiple processes that perform the same computa"
"tion in parallel. Not only that, if we are requiring developers to place `Stre"
"am.async` manually, it may lead to inefficient and error prone solutions."
msgstr ""

msgid ""
"Although the solution above has many flaws, it has helped us ask the right que"
"stions:"
msgstr ""

msgid ""
"  * If `Stream.async` is introducing new processes, how can we guarantee those"
" processes are supervised?"
msgstr ""

msgid ""
"  * Since we are exchanging messages between processes, how do we prevent a pr"
"ocess from receiving too many messages? We need a back-pressure mechanism that"
" allows the receiving process to specify how much it can handle from the sendi"
"ng process."
msgstr ""

msgid ""
"We have jumped through different abstractions trying to answer those questions"
" until we have finally settled on GenStage."
msgstr ""

msgid "## GenStage"
msgstr ""

msgid ""
"GenStage is a new Elixir behaviour for exchanging events with back-pressure be"
"tween Elixir processes. Developers who use GenStage only need to worry about h"
"ow the data is produced, manipulated and consumed. The act of dispatching the "
"data and providing back-pressure is completely abstracted away from the develo"
"pers."
msgstr ""

msgid ""
"As a quick example, let's write a simple pipeline that will produce events as "
"increasing numbers, multiply those numbers by two, and then print them to the "
"terminal. We will do so by implementing three stages, the `:producer`, the `:p"
"roducer_consumer` and the `:consumer`, which we will call `A`, `B` and `C` res"
"pectively. We will go back to the word counting example at the end of this pos"
"t."
msgstr ""

msgid ""
"Let's start with the producer that we will call `A`. Since `A` is a producer, "
"its main responsibility is to receive demand, which is the number of events th"
"e consumer is willing to handle, and generate events. Those events may be in m"
"emory or an external data source. For now let's implement a simple counter sta"
"rting from a given value of `counter` received on `init/1`:"
msgstr ""

msgid ""
"Note: all of the modules in the `GenStage` project are prefixed with the `Expe"
"rimental` namespace. That's why the examples below and your code should `alias"
" Experimental.GenStage` at the top of your files."
msgstr ""

msgid ""
"```elixir\n"
"alias Experimental.GenStage"
msgstr ""

msgid ""
"defmodule A do\n"
"  use GenStage"
msgstr ""

msgid ""
"  def init(counter) do\n"
"    {:producer, counter}\n"
"  end"
msgstr ""

msgid ""
"  def handle_demand(demand, counter) when demand > 0 do\n"
"    # If the counter is 3 and we ask for 2 items, we will\n"
"    # emit the items 3 and 4, and set the state to 5.\n"
"    events = Enum.to_list(counter..counter+demand-1)"
msgstr ""

msgid ""
"    # The events to emit is the second element of the tuple,\n"
"    # the third being the state.\n"
"    {:noreply, events, counter + demand}\n"
"  end\n"
"end\n"
"```"
msgstr ""

msgid ""
"`B` is a producer-consumer. This means it does not explicitly handle the deman"
"d because the demand is always forwarded to its producers. Once `A` receives t"
"he demand from `B`, it will send events to `B` which will be transformed by `B"
"` as desired and then sent to `C`. In our case, B will receive events and mult"
"iply them by a number given on initialization and stored as the state:"
msgstr ""

msgid ""
"defmodule B do\n"
"  use GenStage"
msgstr ""

msgid ""
"  def init(number) do\n"
"    {:producer_consumer, number}\n"
"  end"
msgstr ""

msgid ""
"  def handle_events(events, _from, number) do\n"
"    events = Enum.map(events, & &1 * number)\n"
"    {:noreply, events, number}\n"
"  end\n"
"end\n"
"```"
msgstr ""

msgid ""
"`C` is the consumer which will finally receive those events and print them eve"
"ry second to the terminal:"
msgstr ""

msgid ""
"defmodule C do\n"
"  use GenStage"
msgstr ""

msgid ""
"  def init(sleeping_time) do\n"
"    {:consumer, sleeping_time}\n"
"  end"
msgstr ""

msgid ""
"  def handle_events(events, _from, sleeping_time) do\n"
"    # Print events to terminal.\n"
"    IO.inspect(events)"
msgstr ""

msgid ""
"    # Sleep the configured time.\n"
"    Process.sleep(sleeping_time)"
msgstr ""

msgid ""
"    # We are a consumer, so we never emit events.\n"
"    {:noreply, [], sleeping_time}\n"
"  end\n"
"end\n"
"```"
msgstr ""

msgid "With the stages defined, we can start and connect them:"
msgstr ""

msgid ""
"```elixir\n"
"{:ok, a} = GenStage.start_link(A, 0)    # starting from zero\n"
"{:ok, b} = GenStage.start_link(B, 2)    # multiply by 2\n"
"{:ok, c} = GenStage.start_link(C, 1000) # sleep for a second"
msgstr ""

msgid ""
"GenStage.sync_subscribe(c, to: b)\n"
"GenStage.sync_subscribe(b, to: a)"
msgstr ""

msgid ""
"# Sleep so we see events printed.\n"
"Process.sleep(:infinity)\n"
"```"
msgstr ""

msgid ""
"As soon as we subscribe the stages, we should see items being printed to the t"
"erminal. Notice that, even though we have introduced a sleep command to the co"
"nsumer, the producers will never overflow the consumer with data. That's becau"
"se the communication between stages is demand-driven. The producer can only se"
"nd items to consumers after the consumers have sent demand upstream. The produ"
"cer must never send more items than the consumer has specified."
msgstr ""

msgid ""
"One consequence of this design decision is that parallelizing stateless stages"
" like the consumer above is really straightforward:"
msgstr ""

msgid ""
"```elixir\n"
"{:ok, a} = GenStage.start_link(A, 0)     # starting from zero\n"
"{:ok, b} = GenStage.start_link(B, 2)     # multiply by 2"
msgstr ""

msgid ""
"{:ok, c1} = GenStage.start_link(C, 1000) # sleep for a second\n"
"{:ok, c2} = GenStage.start_link(C, 1000) # sleep for a second\n"
"{:ok, c3} = GenStage.start_link(C, 1000) # sleep for a second\n"
"{:ok, c4} = GenStage.start_link(C, 1000) # sleep for a second"
msgstr ""

msgid ""
"GenStage.sync_subscribe(c1, to: b)\n"
"GenStage.sync_subscribe(c2, to: b)\n"
"GenStage.sync_subscribe(c3, to: b)\n"
"GenStage.sync_subscribe(c4, to: b)\n"
"GenStage.sync_subscribe(b, to: a)"
msgstr ""

msgid ""
"By simply starting multiple consumers, the stage `B` will now receive demand f"
"rom multiple stages and dispatch events to those stages which are now running "
"concurrently, always picking the stage that is able to process more items. We "
"can also leverage concurrency from the opposite direction: if the producer is "
"the slow stage in a pipeline, you can start multiple producers and have each c"
"onsumer subscribe to them."
msgstr ""

msgid ""
"In order to know which consumer should receive a particular event, producer st"
"ages depend on a behaviour called [`GenStage.Dispatcher`](https://hexdocs.pm/g"
"en_stage/Experimental.GenStage.Dispatcher.html). The default dispatcher is the"
" `GenStage.DemandDispatcher` we have briefly described above: it will collect "
"the demand from different consumers and dispatch to the one with highest deman"
"d. This means if one consumer is slow, maybe because we increased its sleeping"
" time to 10 seconds, it will receive less items."
msgstr ""

msgid "### GenStage for data-ingestion"
msgstr ""

msgid ""
"One of the use cases for GenStage is to consume data from third-party systems."
" The demand system with back-pressure guarantees we won't import more data tha"
"n we can effectively handle. The demand dispatcher allows us to easily leverag"
"e concurrency when processing the data by simply adding more consumers."
msgstr ""

msgid ""
"During the Elixir London Meetup, I have live-coded a short example that shows "
"how to use `GenStage` to concurrently process data stored in a PostgreSQL data"
"base as a queue:"
msgstr ""

msgid ""
"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aZuY5-2lwW"
"4\" class=\"video\" allowfullscreen title=\"Elixir London June 2016 w/ José Valim\""
"></iframe>"
msgstr ""

msgid "### GenStage for event dispatching"
msgstr ""

msgid ""
"Another scenario where GenStage can be useful today is to replace cases where "
"developers would have used [GenEvent](https://hexdocs.pm/elixir/GenEvent.html)"
" in the past. For those unfamiliar with GenEvent, it is a behaviour where even"
"ts are sent to an \"event manager\" which then proceeds to invoke \"event handler"
"s\" for each event. GenEvent, however, has one big flaw: the event manager and "
"all event handlers run in the same process. This means GenEvent handlers canno"
"t easily leverage concurrency without forcing developers to implement those me"
"chanisms themselves. Furthermore, GenEvent handlers have very awkward error se"
"mantics. Because event handlers are not separate processes, we cannot simply r"
"ely on supervisors restarting them."
msgstr ""

msgid ""
"GenStage solves those problems by having a producer as the event manager. The "
"producer itself should be configured to use [`GenStage.BroadcastDispatcher`](h"
"ttps://hexdocs.pm/gen_stage/Experimental.GenStage.BroadcastDispatcher.html) as"
" its dispatcher. The broadcast dispatcher will guarantee events are dispatched"
" to all consumers in a way that does not exceed the demand of any of the consu"
"mers. This allows us to leverage concurrency and having the \"event manager\" as"
" a producer gives us much more flexibility in terms of buffering and reacting "
"to failures."
msgstr ""

msgid "Let's see an example of building an event manager as a producer:"
msgstr ""

msgid ""
"defmodule EventManager do\n"
"  use GenStage"
msgstr ""

msgid ""
"  @doc \"\"\"\n"
"  Starts the manager.\n"
"  \"\"\"\n"
"  def start_link() do\n"
"    GenStage.start_link(__MODULE__, :ok, name: __MODULE__)\n"
"  end"
msgstr ""

msgid ""
"  @doc \"\"\"\n"
"  Sends an event and returns only after the event is dispatched.\n"
"  \"\"\"\n"
"  def sync_notify(event, timeout \\\\ 5000) do\n"
"    GenStage.call(__MODULE__, {:notify, event}, timeout)\n"
"  end"
msgstr ""

msgid "  ## Callbacks"
msgstr ""

msgid ""
"  def init(:ok) do\n"
"    {:producer, {:queue.new, 0}, dispatcher: GenStage.BroadcastDispatcher}\n"
"  end"
msgstr ""

msgid ""
"  def handle_call({:notify, event}, from, {queue, demand}) do\n"
"    dispatch_events(:queue.in({from, event}, queue), demand, [])\n"
"  end"
msgstr ""

msgid ""
"  def handle_demand(incoming_demand, {queue, demand}) do\n"
"    dispatch_events(queue, incoming_demand + demand, [])\n"
"  end"
msgstr ""

msgid ""
"  defp dispatch_events(queue, demand, events) do\n"
"    with d when d > 0 <- demand,\n"
"         {item, queue} = :queue.out(queue),\n"
"         {:value, {from, event}} <- item do\n"
"      GenStage.reply(from, :ok)\n"
"      dispatch_events(queue, demand - 1, [event | events])\n"
"    else\n"
"      _ -> {:noreply, Enum.reverse(events), {queue, demand}}\n"
"    end\n"
"  end\n"
"end\n"
"```"
msgstr ""

msgid ""
"The `EventManager` works as a buffer. If there is demand but not events to be "
"sent, we store such demand. If there are events but no demand, we store such e"
"vents in a queue. If a client tries to broadcast an event, the `sync_notify` c"
"all will block until the event is effectively broadcasted. The bulk of the log"
"ic is in the `dispatch_events/3` function that takes events from the queue whi"
"le there is demand."
msgstr ""

msgid ""
"By implementing the event manager as a producer, we can configure all sorts of"
" behaviours that are simply not possible with `GenEvent`, such as how much dat"
"a we want to queue (or for how long) and if events should be buffered or not w"
"hen there are no consumers (via the `handle_subscribe/4` and `handle_cancel/3`"
" callbacks)."
msgstr ""

msgid ""
"Implementing event handlers is as straightforward as writing any other consume"
"r. We could in fact use the `C` consumer implemented earlier. However, given e"
"vent managers are often defined before the handlers, it is recommended for han"
"dlers to subscribe to managers when they start:"
msgstr ""

msgid ""
"defmodule EventHandler do\n"
"  use GenStage"
msgstr ""

msgid ""
"  def start_link() do\n"
"    GenStage.start_link(__MODULE__, :ok)\n"
"  end"
msgstr ""

msgid "  # Callbacks"
msgstr ""

msgid ""
"  def init(:ok) do\n"
"    # Starts a permanent subscription to the broadcaster\n"
"    # which will automatically start requesting items.\n"
"    {:consumer, :ok, subscribe_to: [EventManager]}\n"
"  end"
msgstr ""

msgid ""
"  def handle_events(events, _from, state) do\n"
"    IO.inspect events\n"
"    {:noreply, [], state}\n"
"  end\n"
"end\n"
"```"
msgstr ""

msgid ""
"Such guarantees that, if a supervised `EventHandler` crashes, the supervisor w"
"ill start a new event handler which will promptly subscribe to the same manage"
"r, solving the awkward error handling semantics we have seen with `GenEvent`."
msgstr ""

msgid "## The path forward"
msgstr ""

msgid ""
"With the release of GenStage v0.3.0, we have reached an important milestone as"
" `GenStage` can be used as both event managers and a way to exchange events be"
"tween processes, often external data sources, with back-pressure."
msgstr ""

msgid ""
"The v0.3.0 release also includes the [`GenStage.stream`](https://hexdocs.pm/ge"
"n_stage/Experimental.GenStage.html#stream/1) function, which allows us to cons"
"ume data from a GenStage as a stream, and [`GenStage.from_enumerable`](https:/"
"/hexdocs.pm/gen_stage/Experimental.GenStage.html#from_enumerable/2) which allo"
"ws us to use an enumerable or a stream, like `File.stream!`, as a producer. Cl"
"osing the gap between stages and streams."
msgstr ""

msgid "However, we are far from done!"
msgstr ""

msgid ""
"First of all, now is the moment for the community to step in and try GenStage "
"out. If you have used GenEvent in the past, can it be replaced by a GenStage? "
"Similarly, if you were planning to implement an event handling system, give Ge"
"nStage a try."
msgstr ""

msgid ""
"Developers who maintain libraries that integrate with external data sources, b"
"e it a RabbitMQ, Redis or Apacha Kafka, can explore GenStage as an abstraction"
" for consuming data from those sources. Library developers must implement prod"
"ucers and leave it up for their users to configure the consumer stages."
msgstr ""

msgid ""
"Once we get enough feedback, `GenStage` will be included in some shape as part"
" of the standard library. The goal is to introduce `GenStage` and phase `GenEv"
"ent` out in the long term."
msgstr ""

msgid ""
"We, on the Elixir team, have just got started too. The next milestone for GenS"
"tage is to revisit the original problem and provide developers a clear path to"
" take their collection processing code from eager, to lazy, to concurrent (and"
" then distributed)."
msgstr ""

msgid ""
"As seen earlier, today we allow developers to transform eager code into lazy b"
"y introducing streams."
msgstr ""

msgid ""
"While the above is helpful when working with large or infinite collections, it"
" still does not leverage concurrency. To address that, we are currently explor"
"ing a solution named [`GenStage.Flow`](https://hexdocs.pm/gen_stage/Experiment"
"al.Flow.html), that allows us to express our computations similarly to streams"
", except they will run across multiple stages instead of a single process:"
msgstr ""

msgid ""
"```elixir\n"
"alias Experimental.GenStage.Flow\n"
"File.stream!(\"path/to/some/file\")\n"
"|> Flow.from_enumerable()\n"
"|> Flow.flat_map(fn line ->\n"
"    for word <- String.split(\" \"), do: {word, 1}\n"
"   end)\n"
"|> Flow.reduce_by_key(& &1 + &2)\n"
"|> Enum.to_list()\n"
"```"
msgstr ""

msgid "And the highly optimized version:"
msgstr ""

msgid ""
"```elixir\n"
"alias Experimental.GenStage.Flow"
msgstr ""

msgid ""
"# Let's compile common patterns for performance\n"
"empty_space = :binary.compile_pattern(\" \") # NEW!"
msgstr ""

msgid ""
"File.stream!(\"path/to/some/file\", read_ahead: 100_000) # NEW!\n"
"|> Flow.from_enumerable()\n"
"|> Flow.flat_map(fn line ->\n"
"    for word <- String.split(empty_space), do: {word, 1}\n"
"   end)\n"
"|> Flow.partition_with(storage: :ets) # NEW!\n"
"|> Flow.reduce_by_key(& &1 + &2)\n"
"|> Enum.to_list()\n"
"```"
msgstr ""

msgid ""
"Flow will look at the computations we want to perform and start a series of st"
"ages to execute our code while keeping the amount of data being transfered bet"
"ween processes to a minimum. If you are interested in `GenStage.Flow` and how "
"the computations above are spread across multiple stages, [we have written som"
"e documentation based on the prototypes we have built so far](https://hexdocs."
"pm/gen_stage/Experimental.Flow.html). The code itself is coming in future GenS"
"tage releases. We will also have to consider how the `GenStage.Flow` API mirro"
"rs the functions in `Enum` and `Stream` to make the path from eager to concurr"
"ent clearer."
msgstr ""

msgid ""
"For the word counting problem with a fixed data, early experiments show a line"
"ar increase in performance with a fixed overhead of 20%. In other words, a dat"
"aset that takes 60s with a single core, takes 36s on a machine with 2 cores an"
"d 18s in one with 4 cores. All of those gains by simply moving your computatio"
"ns from streams to Flow. We plan to benchmark on machines with over 40 cores s"
"oon."
msgstr ""

msgid ""
"We are very excited with the possibilities GenStage brings to developers and a"
"ll new paths it allows us to explore and research. So give it a try and let us"
" know! [GenStage, Flows, and more will also be the topic of my keynote at Elix"
"irConf 2016](http://www.elixirconf.com/) and we hope to see you there."
msgstr ""

msgid ""
"Finally, we want to thank the [akka-streams and reactive-streams projects](htt"
"p://reactive-streams.io) which provided us guidance in implementing the demand"
"-driven exchange between stages as well as the [Apache Spark](http://spark.apa"
"che.org/) and [Apache Beam](http://beam.incubator.apache.org/) initiatives tha"
"t inspire the work behind `GenStage.Flow`."
msgstr ""

msgid "Happy coding!"
msgstr ""
