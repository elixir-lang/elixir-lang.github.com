---
layout: getting_started
title: 10 Distributed tasks and configuration
guide: 10
last: true
---

# {{ page.title }}

  <div class="toc"></div>

In this last chapter, we will go back to the `:kv` application and add a routing layer that allows us to distribute requests in between nodes based on the bucket name.

The routing layer will receive a routing table of the following format:

    [{?a..?m, :"foo@computer-name"},
     {?n..?z, :"bar@computer-name"}]

The router will check the first byte of the bucket name against the table and dispatch to the appropriate node based on that. For example, a bucket starting with the letter `?a` will be dispatched to node `foo@computer-name`.

In case the matching entry points to the current node itself, we are done. Otherwise the next node will receive the bucket name, look at its own routing table (which may be different from the one in the first node) and act accordingly. In case no entry matches, an error will be raised.

## 10.1 Our first distributed code

Elixir ships with facilities to connect nodes and exchange information in between them. In fact, we use the same concepts of processes, message passing and receiving messages when working on a distributed environment because we say Elixir processes are *location transparent*. When sending a message, it doesn't matter if the recipient process is on the same node or in another node, the VM will be able to deliver the message in both cases.

In order to run distributed code, we need to start the VM with a name. The name can be short (when in the same network) or long (requires the full computer address). Let's start a new IEx session:

    $ iex --sname foo

You can see now the prompt is slightly different and shows the node name followed by the computer name:

    Interactive Elixir - press Ctrl+C to exit (type h() ENTER for help)
    iex(foo@jv)1>

My computer is named `jv`, that's why we see `foo@jv` in the example above, but you should get a different result. We will use `jv@computer-name` in the following examples and you should update them accordingly when trying out the code.

Let's define a module named `HelloWorld` in this shell:

```iex
iex> defmodule Hello do
...>  def world, do: IO.puts "hello world"
...> end
```

If you have another computer in the same network with both Erlang and Elixir installed, you can start another shell on it. If you don't, you can simply start another IEx session in another terminal and give it the short name of `bar`:

    $ iex --sname bar

Note that inside this new IEx session, we cannot access `Hello.world/0`:

```iex
iex> Hello.world
** (UndefinedFunctionError) undefined function: Hello.world/0
    Hello.world()
```

However we can spawn a new process in `foo@computer-name` from `bar@computer-name`! Let's give it a try (where `@computer-name` is the one you see locally):

```iex
iex> Node.spawn_link :"foo@computer-name", fn -> Hello.world end
#PID<9014.59.0>
hello world
```

Elixir spawned a process in another node and returned its pid. The code then executed in the other node where the `Hello.world/0` function exists and invoked that function. Note the result of "hello world" was printed in the current node `bar` and not in `foo`. In other words, the message to be printed was sent back from `foo` to `bar`. This happens because the process spawned in the other node (`foo`) still has the group leader of the current node (`bar`). We have briefly talked about group leaders in the [IO chapter](/getting_started/12.html).

We can send and receive message from the pid returned by `Node.spawn_link/2` as usual. Let's try a quick ping-pong example:

```iex
iex> pid = Node.spawn_link :"foo@computer-name", fn ->
...>   receive do
...>     {:ping, client} -> send client, :pong
...>   end
...> end
#PID<9014.59.0>
iex> send pid, {:ping, self}
{:ping, #PID<0.73.0>}
iex> flush
:pong
:ok
```

From our quick exploration, we could conclude we could simply use `Node.spawn_link/2` to spawn entries in a remote node every time we need to do a distributed computation. However we have learned throughout the guide that spawning processes outside of supervision trees should be avoided if possible, so we need to look for other options.

There are three better alternatives to `Node.spawn_link/2` we could use in our implementation:

1. We could use Erlang's [:rpc](http://erlang.org/doc/man/rpc.html) module to execute functions in a remote node. Inside the `bar@computer-name` above, you can call `:rpc.call :"foo@computer-name", Hello, :world, []` and it will print "hello world";

2. Another alternative is to have a server running in the other node and send requests to that node via the [GenServer](/docs/stable/elixir/GenServer.html) API. For example, you can call a remote named server using `GenServer.call({name, node}, arg)` or simply passing the remote process PID as first argument;

3. A third option is to use tasks, which we have learned in the previous chapter, as they support spawning tasks both on local and remote nodes;

The options above have different properties. Both `:rpc` and using a GenServer would serialize your requests into a single server, while tasks are effecticely running in asynchronously in the remote node, with the only serialization point being the spawning done by the supervisor.

For our routing layer, we are going to use tasks, but feel free to explore the other alternatives too.

## 10.2 async/await

So far we have explored tasks that are started and run in isolation, with no regard to its return value. However sometimes it is useful to run a task to compute a value and read its result later on. For such, tasks also provide the `async/await` pattern:

```elixir
task = Task.async(fn -> compute_something_expensive end)
res  = compute_something_else()
res + Task.await(task)
```

`async/await` provides a very simple mechanism to compute values concurrently. Not only that, `async/await` can also be used with the same [`Task.Supervisor`](/docs/stable/elixir/Task.Supervisor.html) we have used in previous chapters, we just need to call `Task.Supervisor.async/2` instead of `Task.Supervisor.start_child/2` and use `Task.await/2` to read the result later on.

## 10.3 Distributed tasks

Distributed tasks are exactly the same as supervised tasks. The only difference is that we pass the node name when spawning the task in the supervisor. Open up `lib/kv/supervisor.ex` from the `:kv` application and let's add a task supervisor to the tree:

```elixir
supervisor(Task.Supervisor, [[name: KV.RouterTasks]]),
```

Now let's start two named nodes again but inside the `:kv` application:

    $ iex --sname foo -S mix
    $ iex --sname bar -S mix

From inside `bar@cmputer-name`, we can now spawn a task directly in the other node via the supervisor:

```iex
iex> task = Task.Supervisor.async {KV.RouterTasks, :"foo@computer-name"}, fn ->
...>   {:ok, node()}
...> end
%Task{pid: #PID<12467.88.0>, ref: #Reference<0.0.0.400>}
iex> Task.await(task)
{:ok, :"foo@computer-name"}
```

Our first distributed task is quite-straightforward, it simply gets the name of the node the task is running on. With this knowledge in hand, let's finally write the routing code.

## 10.4 Routing layer

Create a file at `lib/kv/router.ex` with the following contents:

```elixir
defmodule KV.Router do
  @doc """
  Dispatch the given `mod`, `fun`, `args` request
  to the appropriate node according to `bucket`.
  """
  def route(bucket, mod, fun, args) do
    # Get the first byte of binary
    first = :binary.first(bucket)

    # Try to find an entry in the table or raise
    entry =
      Enum.find(table, fn {enum, node} ->
        first in enum
      end) || no_entry_error(bucket)

    # If the entry node is the current node
    if elem(entry, 1) == node() do
      apply(mod, fun, args)
    else
      sup = {KV.RouterTasks, elem(entry, 1)}
      Task.Supervisor.async(sup, fn ->
        KV.Router.route(bucket, mod, fun, args)
      end) |> Task.await()
    end
  end

  defp no_entry_error(bucket) do
    raise "could not find entry for #{inspect bucket} in table #{inspect table}"
  end

  @doc """
  The routing table.
  """
  def table do
    # Replace computer-name by your local machine nodes.
    [{?a..?m, :"foo@computer-name"},
     {?n..?z, :"bar@computer-name"}]
  end
end
```

Let's write a test to verify our router works. Create a file named `test/kv/router_test.exs` with:

```elixir
defmodule KV.RouterTest do
  use ExUnit.Case, async: true

  test "route requests accross nodes" do
    assert KV.Router.route("hello", Kernel, :node, []) ==
           :"foo@computer-name"
    assert KV.Router.route("world", Kernel, :node, []) ==
           :"bar@computer-name"
  end

  test "raises on unknown entries" do
    assert_raise RuntimeError, ~r/could not find entry/, fn ->
      KV.Router.route(<<0>>, Kernel, :node, [])
    end
  end
end
```

The first test simply invokes `Kernel.node/0`, which returns the name of the current node, based on the bucket names "hello" and "world". According to our routing table so far, we should get `foo@computer-name` and `bar@computer-name` as responses respectively.

The second test just checks the code raises for unknown entries.

In order to run the first test, we need to have two nodes running. Let's restart the node named `bar` which is going to be used by tests:

    $ iex --sname bar -S mix

And now run tests with:

    $ elixir --sname foo -S mix test

Our test should successfuly pass, excellent!

## 10.5 Test filters and tags

Although our tests pass, our testing structure is getting more complex. In particular, running tests simply with `mix test` cause failures in our suite since our test requires connection to other nodes.

Luckily, ExUnit ships with a facility to tag tests, allowing us to run specific callbacks or even filter tests altogether based on those tags.

All we need to do to tag a test is simply call `@tag` before the test name. Back to `test/kv/routest_test.exs`, let's add a `:distributed` tag:

```elixir
@tag :distributed
test "route requests accross nodes" do
```

`@tag :distributed` is equivalent to `@tag distributed: true`.

With the test properly tagged, we can now check if the node is alive on the network and, if not, we can exclude all distributed tests. Open up `test/test_helper.exs` inside the `:kv` application and add the following:

```elixir
exclude =
  if Node.alive?, do: [], else: [distributed: true]

ExUnit.start(exclude: exclude)
```

Now run tests with `mix test`:

    $ mix test
    Excluding tags: [distributed: true]

    .......

    Finished in 0.1 seconds (0.1s on load, 0.01s on tests)
    7 tests, 0 failures

This time all tests passed and ExUnit warned us that distributed tests were being excluded. If you run tests with `$ elixir --sname foo -S mix test`, one extra test shuold run and successfully pass as long as the `bar@computer-name` node is available.

The `mix test` command also allows us to dynamically include and exclude tags. For example, we can run `$ mix test --include distributed` to run distributed tests regardless of the value set in `test/test_helper.exs`. We could also pass `--exclude` to exclude a particular tag from the command line. Finally, `--only` can be used to run only tests with a particular tag:

    $ elixir --sname foo -S mix test --only distributed`

You can read more about filters, tags and the default tags in [`ExUnit.Case` module documentation](/docs/stable/ex_unit/ExUnit.Case.html).

## 10.6 Application environment and configuration

So far we have hardcoded the routing table into the `KV.Router` module. However, we would like to make the table dynamic so not only we can configure it development/test/production but also allow different nodes to run with different entries in the routing table. There is a convenience in OTP that does exactly that: the application environment.

Each application has an environment that stores the application specific configuration by key. For example, we could store the routing table in the `:kv` application environment, giving it a default value and allowing other applications to change the table as needed.

Open up `apps/kv/mix.exs` and change the `application/0` function to return the following:

```elixir
def application do
  [applications: [],
   env: [routing_table: []],
   mod: {KV, []}]
end
```

We have added a new `:env` key to the application, that returns the application default environment which has an entry of key `:routing_table` and value of an empty list. It makes sense for the application environment to ship with an empty table, as the specific of the routing table depends on the testing/deployment structure.

In order to use the application environment in our code, we just need to replace `KV.Router.table/0` by the definition below:

```elixir
@doc """
The routing table.
"""
def table do
  Application.get_env(:kv, :routing_table)
end
```

We use `Application.get_env/2` to read the entry for `:routing_table` in `:kv`'s environment. You can find more information and other functions to manipulate the app environment in the [Application module](/docs/stable/elixir/Application.html).

Since our routing table is now empty, our distributed test should fail. Restart the apps and re-run tests to see the failure:

    $ iex --sname bar -S mix
    $ elixir --sname foo -S mix test --only distributed

The interesting thing about the application environment is that it can be configured, not only for the current application, but for all other applications. Such configuration is done by the `config/config.exs` file. For example, we can configure IEx default prompt to another value. Just open `apps/kv/config/config.exs` and add the following to the end:

```elixir
config :iex, default_prompt: ">>>"
```

Start IEx with `iex -S mix` and you can see IEx prompt has changed.

This means we can configure our `:routing_table` directly in the `config/config.exs` file as well:

```elixir
# Replace computer-name by your local machine nodes.
config :kv, :routing_table,
       [{?a..?m, :"foo@computer-name"},
        {?n..?z, :"bar@computer-name"}]
```

Restart the nodes and run distributed tests again and now they should all pass.

Each application has their `config/config.exs` file and they are not shared in any way. Configuration can also be set per environment, read the contents of the config file for the `:kv` application for more information on that.

Since config files are not shared, if you run tests from the umbrella root, they will fail because the configuration we have just added to `:kv` is not available there. However, if you open up `config/config.exs` in the umbrella, it has instructions on how to import config files from children applications. You just need to invoke:

```elixir
import_config "../apps/kv/config/config.exs"
```

The `mix run` command also accept a `--config` flag, which allows configuration files to be given on demand. This could be used to start different nodes, each with their specific configuration (for example, different routing tables).

Overall, the built-in ability to configure applications and the fact we have built our software as an umbrella application gives us plenty of options when deploying the software. We can:

* deploy the umbrella application to a node that will work as both TCP server and key-value storage;

* deploy the `:kv_server` application to work only as a TCP server as long as the routing table points only to other nodes;

* deploy only the `:kv` application when we want a node to work only as storage (no TCP access);

Once you add more applications in the future, you can continue controlling your deploy with the same level of granularity, cherry-picking which applications with which configuration are going to production.

Finally, we have learned some new things in this chapter and they could be applied to the `:kv_server` application as well. We are going to leave the next steps as an exercise:

* change the `:kv_server` application to read the port from its application environment instead of using the hardcoded value of 4040;

* change and configure the `:kv_server` application to use the routing functionality instead of dispatching directly to the local `KV.Registry`. For `:kv_server` tests, you can make the routing table simply point to the current node itself;

## 10.7 Summing up

In this chapter we have built a simple router as a way to explore the distributed features in Elixir and in the Erlang VM and learned how to configure its routing table. This is the last chapter in our Mix and OTP guide.

Throughout the guide, we have built a very simple distributed key-value store as an opportunity to explore many constructs like gen servers, event managers, supervisors, tasks, agents, applications and more. Not only that, we have written tests for the whole application, getting familiar with ExUnit, and learned how to use the Mix build tool to accomplish a wide range of tasks.

In case you are looking for a distributed key-value store to use in production, you should definitely look into [Riak](http://basho.com/riak/) which also runs in the Erlang VM. In Riak, the buckets are replicated, to avoid data loss, and instead of a router, they use [consistent hashing](http://en.wikipedia.org/wiki/Consistent_hashing) to map a bucket to a node. A consistent hashing algorithm helps reduce the amount of data that needs to be migrated when new nodes to store buckets are added to your infrastructure.
